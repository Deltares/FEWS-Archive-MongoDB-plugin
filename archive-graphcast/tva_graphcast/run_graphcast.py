#  Copyright (c) 2024 INFISYS INC

import os
import json
import html
import logging
import argparse
import traceback
import xmltodict

import numpy as np
import pandas as pd

from tva_graphcast.decorator import log
from tva_graphcast.sources import Ifs, Gfs
from datetime import datetime, timedelta

_parser = argparse.ArgumentParser()
_parser.add_argument('--config_path', '-c', dest='config_path', type=str, help='The fully qualified path to the configuration file', required=True)
_args = _parser.parse_args()

_timestep = 6
_observed_timesteps = 2

_metadata = {
	'variables': {
		'lat': {
			'units': 'degrees north',
			'long_name': 'latitude',
			'standard_name': 'latitude'
		},
		'lon': {
			'units': 'degrees east',
			'long_name': 'longitude',
			'standard_name': 'longitude'
		},
		'level': {
			'units': 'hPa',
			'long_name': 'isobaric surface pressure',
			'positive': 'down',
			'stored_direction': 'decreasing',
			'standard_name': 'air_pressure'
		},
		'2m_temperature': {
			'units': 'K',
			'long_name': 'temperature @ 2 meters above ground',
			'abbreviation': 'TMP',
			'netcdf_statistical_interval_type': 'instantaneous'
		},
		'10m_u_component_of_wind': {
			'units': 'm/s',
			'long_name': 'u-component of wind @ 10 m above ground',
			'abbreviation': 'UGRD',
			'netcdf_statistical_interval_type': 'instantaneous'
		},
		'10m_v_component_of_wind': {
			'units': 'm/s',
			'long_name': 'v-component of wind @ 10 m above ground',
			'abbreviation': 'VGRD',
			'netcdf_statistical_interval_type': 'instantaneous'
		},
		'total_precipitation_6hr': {
			'units': 'kg.m-2',
			'long_name': 'total precipitation (6_Hour accumulation) @ ground or water surface',
			'abbreviation': 'APCP',
			'netcdf_statistical_interval_type': 'accumulation'
		},
		'mean_sea_level_pressure': {
			'units': 'Pa',
			'long_name': 'pressure reduced to MSL @ mean sea level',
			'abbreviation': 'PRMSL',
			'netcdf_statistical_interval_type': 'instantaneous'
		},
		'temperature': {
			'units': 'K',
			'long_name': 'temperature @ isobaric surface',
			'abbreviation': 'TMP',
			'netcdf_statistical_interval_type': 'instantaneous'
		},
		'geopotential': {
			'units': 'm2.s-2',
			'long_name': 'geopotential height @ isobaric surface',
			'abbreviation': 'HGT',
			'netcdf_statistical_interval_type': 'instantaneous'
		},
		'specific_humidity': {
			'units': 'kg.kg',
			'long_name': 'specific humidity @ isobaric surface',
			'abbreviation': 'SPFH',
			'netcdf_statistical_interval_type': 'instantaneous'
		},
		'vertical_velocity': {
			'units': 'Pa/s',
			'long_name': 'vertical velocity (pressure) @ isobaric surface',
			'abbreviation': 'VVEL',
			'netcdf_statistical_interval_type': 'instantaneous'
		},
		'u_component_of_wind': {
			'units': 'm/s',
			'long_name': 'u-component of wind @ isobaric surface',
			'abbreviation': 'UGRD',
			'netcdf_statistical_interval_type': 'instantaneous'
		},
		'v_component_of_wind': {
			'units': 'm/s',
			'long_name': 'v-component of wind @ isobaric surface',
			'abbreviation': 'VGRD',
			'netcdf_statistical_interval_type': 'instantaneous'
		}
	},
	'global': {
		'originating_or_generating_center': 'Tennessee Valley Authority (TVA)',
		'originating_or_generating_sub_center': 'River Management',
		'type_of_generating_process': 'forecast',
		'analysis_or_forecast_generating_process_identifier_defined_by_originating_center': 'Quantitative Precipitation Forecast Generated by TVA',
		'file_format': 'NetCDF',
		'conventions': ['CF-1.6', 'COARDS'],
		'feature_type': 'GRID'
	}
}

_diag = '''<?xml version="1.0" encoding="UTF-8"?>
<Diag xmlns="http://www.wldelft.nl/fews/PI" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.wldelft.nl/fews/PI http://fews.wldelft.nl/schemas/version1.0/pi-schemas/pi_diag.xsd" version="1.2">
	<line level="1" description="{description}" />
</Diag>'''


def main():
	logging_format = '{"time": "%(asctime)s", "level": "%(levelname)s", "name": "%(name)s", "message": "%(message)s"}'
	logging.basicConfig(filename=os.path.join(os.path.dirname(__file__), 'tva_graphcast.log'), level=logging.INFO, filemode='w+', format=logging_format)
	stream_handler = logging.StreamHandler()
	stream_handler.setFormatter(logging.Formatter(logging_format))
	stream_handler.setLevel(logging.INFO)
	logging.getLogger().addHandler(stream_handler)
	RunGraphcast().main()


class RunGraphcast:

	def __init__(self):
		with open(_args.config_path, 'r') as f:
			config = xmltodict.parse(f.read())['Run']
		properties = {c['@key']: c['@value'] for c in config['properties']['string']}

		self.output_diag_file = config['outputDiagnosticFile']
		self.t0 = datetime.strptime(f"{config['time0']['@date']}T{config['time0']['@time']}", "%Y-%m-%dT%H:%M:%S")

		self.t0_offset_timesteps = int(properties['time_zero_offset_timesteps'])
		self.predictions = json.loads(properties['forecast_length_timesteps'])
		self.levels = json.loads(properties['output_levels'])
		self.fields = json.loads(properties['output_parameters'].replace('\'', '"'))
		self.corners = json.loads(properties['output_bounding_box'])
		self.output_folder = os.path.normpath(os.path.expandvars(properties['output_folder']))
		self.output_file = os.path.join(self.output_folder, properties['output_filename'])
		self.model_path = os.path.expandvars(properties['model_path'])
		self.source = properties['source_initialization']
		self.downscale = float(properties['downscaled_grid_cell_size_deg'])
		self.downscale_method = properties['downscale_method']

	@log
	def main(self):

		try:
			t0 = self.t0 + timedelta(hours=self.t0_offset_timesteps * _timestep)
			if self.source == 'ifs':
				results = Ifs.model(t0, self.predictions, self.model_path, _timestep, _observed_timesteps)
			elif self.source == 'gfs':
				results = Gfs.model(t0, self.predictions, self.model_path, _timestep, _observed_timesteps)
			else:
				raise ValueError(f'source_initialization: {self.source}')

			results = RunGraphcast._ensure_lat_lon(results)
			results = RunGraphcast._trim(results, self.levels, self.fields)
			results = RunGraphcast._clip(results, self.corners)
			results = RunGraphcast._resample(results, self.downscale, self.downscale_method)
			results = RunGraphcast._zero_negative_precipitation(results)
			results = RunGraphcast._metadata(results)
			results = RunGraphcast._update_datetime(results)
			single_level_results, pressure_levels_results = RunGraphcast._split(results)

			os.makedirs(self.output_folder, exist_ok=True)
			output_file_parts = self.output_file.split('.')

			RunGraphcast._save(single_level_results, f"{'.'.join(output_file_parts[:-1])}.SingleLevel.{t0:%Y%m%d%H%M}.{output_file_parts[-1]}")
			RunGraphcast._save(pressure_levels_results, f"{'.'.join(output_file_parts[:-1])}.PressureLevels.{t0:%Y%m%d%H%M}.{output_file_parts[-1]}")

		except Exception as e:
			with open(self.output_diag_file, 'w') as f:
				f.write(str.format(_diag, description=RunGraphcast._escape(traceback.format_exc())))
			raise e

	@staticmethod
	@log
	def _update_datetime(x):
		return x.assign_coords(time=pd.to_datetime(x.coords['time']))

	@staticmethod
	@log
	def _zero_negative_precipitation(x):
		x['total_precipitation_6hr'] = x['total_precipitation_6hr'].where(x['total_precipitation_6hr'] >= 0, 0)
		return x

	@staticmethod
	@log
	def _save(x, file):
		output_format = file.split('.')[-1]
		if output_format in ['nc', 'netcdf']:
			x.to_netcdf(file, format='NETCDF4')
		elif output_format == 'zarr':
			x.to_zarr(file, mode='w')
		elif output_format == 'json':
			with open(file, 'w') as w:
				json.dump(x.to_dict(), w, default=str)
		elif output_format in ['txt', 'csv']:
			x.to_dataframe().to_csv(file)

	@staticmethod
	@log
	def _metadata(x):
		for r in x.variables:
			if r in _metadata['variables']:
				for k, v in _metadata['variables'][r].items():
					x[r].attrs[k] = v
		for k, v in _metadata['global'].items():
			x.attrs[k] = v
		return x

	@staticmethod
	@log
	def _ensure_lat_lon(x):
		x['lon'] = np.where(x.lon >= 0, x.lon, x.lon + 360)
		return x

	@staticmethod
	@log
	def _trim(x, levels, fields):
		return x[fields].sel(level=levels).squeeze(['batch'])

	@staticmethod
	@log
	def _clip(x, corners):
		min_lon = min([c[0] for c in corners])
		min_lon = min_lon + 360 if min_lon < 0 else min_lon

		max_lon = max([c[0] for c in corners])
		max_lon = max_lon + 360 if max_lon < 0 else max_lon

		return x.where((x.lon >= min_lon) & (x.lon <= max_lon) & (x.lat >= min([c[1] for c in corners])) & (x.lat <= max([c[1] for c in corners])), drop=True)

	@staticmethod
	@log
	def _resample(x, downscale, downscale_method):
		lat_min, lat_max = x.lat.values.min(), x.lat.values.max()
		lon_min, lon_max = x.lon.values.min(), x.lon.values.max()
		return x.interp({'lat': np.arange(lat_min, lat_max, downscale), 'lon': np.arange(lon_min, lon_max, downscale)}, downscale_method)

	@staticmethod
	@log
	def _split(x):
		splits = {tuple(x[v].dims): [] for v in x.data_vars}
		[splits[tuple(x[v].dims)].append(v) for v in x.data_vars]
		return x[splits[('time', 'lat', 'lon')]], x[splits[('time', 'lat', 'lon', 'level')]]

	@staticmethod
	def _escape(s):
		return html.escape(s)


if __name__ == '__main__':
	main()
